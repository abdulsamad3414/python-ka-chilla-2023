{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52c1e92",
   "metadata": {},
   "source": [
    "# Activation Function \n",
    "\n",
    "convert linear function to non linearn function and convert it into classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a3048",
   "metadata": {},
   "source": [
    "# Types of Activation Functions\n",
    "\n",
    "Linear activation Function \n",
    "\n",
    "step activation function \n",
    "\n",
    "sigmoid activation function \n",
    "\n",
    "tanH activation Function \n",
    "\n",
    "Relu Activation Function \n",
    "\n",
    "Softmax Activation Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5ad65",
   "metadata": {},
   "source": [
    "# Why need activation function ?\n",
    "\n",
    "Data is Non Structure \n",
    "\n",
    "No define line between useful and non useful data \n",
    "\n",
    "Noisy data will not give reqiured output\n",
    "\n",
    "simply is that jo bi noisy data a raha ha usy set karnay ma help karta ha "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e7e00",
   "metadata": {},
   "source": [
    "# Linear Activation function \n",
    "\n",
    "input leta ha or output deta ha output linear hi hoga input ma kitna hi change karalain multiple karnay pa bi linear hi banayga \n",
    "\n",
    "# Non Linear Activation Function \n",
    "\n",
    "wo hota ha jis may ham input parameters ma chnages kartay hain to hamara output sai hojata ha because uskay under activation function add hota ha means agaroutput hamary mutabik nea ata to ham backpropagation kartay hain or x oe weigth ki values ko adjust kartay hain\n",
    "\n",
    " 1. Binary activation function \n",
    "   \n",
    "     it just give us yes or no \n",
    "\n",
    "2. Sigmoid function \n",
    "    \n",
    "\n",
    "    no direct jump due to s shape ,but drawback ya ha ka ya vanishing gredient hoty hain means 0 to 1 kay between values ko pridict nea kar patay and range is 0 to 1 highest is 1 and lowest is 0,ya zero centered nea ha \n",
    "\n",
    "\n",
    "3. Tangent H function \n",
    "   1. is shape bi s ki tarha hoti ha but range is -1 to 1 \n",
    "   2. zero centered hai\n",
    "   3. but it is alse a gredient vanishing like a sigmoid function\n",
    "4. Relu function \n",
    "   1. iska derivative function bi ha or iski range 0 to infinite ha \n",
    "   2. allow backpropagation means ya correction kar sukta ha weighted sum and biseness kay under\n",
    "   3. ya sary nueral function ko ak hi sath check nea karta ha jismay error hota ha just usy check karta ha \n",
    "   4. f(x)=max(0x,a)\n",
    "   5. best fit for positive values \n",
    "5. Leaky Relu Function \n",
    "   1. it takes all negative values jo kay relu function ma nea thi\n",
    "   2. ismay ham 0.1 percent lety hain \n",
    "   3. f(x)=max(0.1x,a)\n",
    "   4. best fit for positive plus 5 to 10% negative values \n",
    "6. Parametric Relu \n",
    "   1. f(x)=max(ax,a)\n",
    "   2. means takes all negative values and it adjust itself \n",
    "   3. best fit for 70% negative and 50 % positive values \n",
    "7. Softmax function \n",
    "   1. ia may sub probality ka sum equal to 1 hota ha or jis ki probablity high hoti ha wohi answer ata ha ismay \n",
    "   2. it can be use for multi classification like dog ,cat ,elephant is ki image agar predict karna hoto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd5bf7",
   "metadata": {},
   "source": [
    "# How to choose Activation Function?\n",
    "\n",
    "1. Use sigmoid function in output layer \n",
    "2. tanh use for hiden layers \n",
    "3. Relu use for both output and hiden layer but mostly use in hiden layers \n",
    "\n",
    "* **Hiden layer**\n",
    "  1. jo kay input layer say data le or output ko day "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31638079",
   "metadata": {},
   "source": [
    "# Neural Network  Type (Hidden Layer Activation Functions)\n",
    "\n",
    "Multilinear Perceptron \n",
    "1. Relu Function \n",
    "\n",
    "Convolutional Neural Networks\n",
    "\n",
    "\n",
    "1. Relu function \n",
    "\n",
    "\n",
    "Recurrent Neural Network \n",
    "\n",
    "\n",
    "1. Tanh and sigmoid Function use karaygay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370c890",
   "metadata": {},
   "source": [
    "# Output functions \n",
    "\n",
    "1. Linear Activation Function \n",
    "2. Logistic (Sigmod) Function \n",
    "3. Softmax Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326123e3",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "For Regression Problem use lINEAR aCTIVATION fUNCTION \n",
    "\n",
    "FOR cLASSFICATION PROBLEM USE THE SIGMOID AND LOGIDTIC FUNCTION \n",
    "\n",
    "FOR CLASSIFICATION THERE ARE THREE OUTPUT TYPES \n",
    "1. bINARY CLASSIFICATION .....USE SIGMOId function \n",
    "2. multiclass classification ....use softmax function \n",
    "3. Multilabel Classification .......use also Sigmoid function "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
